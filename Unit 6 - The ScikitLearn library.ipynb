{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ae012f",
   "metadata": {},
   "source": [
    "# The ScikitLearn.jl library\n",
    "\n",
    "The Scikit-learn library is an open source machine learning library developed for the Python programming language, the first version of which dates back to 2010. It implements a large number of machine learning models, related to tasks such as classification, regression, clustering or dimensionality reduction. These models include Support Vector Machines (SVM), decision trees, random forests, or k-means. It is currently one of the most widely used libraries in the field of machine learning, due to the large number of functionalities it offers as well as its ease of use, since it provides a uniform interface for training and using models. The documentation for this library is available at https://scikit-learn.org/stable/.\n",
    "\n",
    "For Julia, the ScikitLearn.jl library implements this interface and the algorithms contained in the scikit-learn library, supporting both Julia's own models and those of the scikit-learn library. The latter is done by means of the PyCall.jl library, which allows code written in Python to be executed from Julia in a transparent way for the user, who only needs to have ScikitLearn.jl installed. Documentation for this library can be found at https://scikitlearnjl.readthedocs.io/en/latest/.\n",
    "\n",
    "As mentioned above, this library provides a uniform interface for training different models. This is reflected in the fact that the names of the functions for creating and training models will be the same regardless of the models to be developed. In the assignments of this course, in addition to ANNs, the following models available in the scikit-learn library will be used:\n",
    "\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision trees\n",
    "- kNN\n",
    "\n",
    "In order to use these models, it is first necessary to import the library (using ScikitLearn, which must be previously installed with\n",
    "\n",
    "```Julia\n",
    "import Pkg;\n",
    "Pkg.add(\"ScikitLearn\"))\n",
    "```\n",
    "\n",
    "The scikit-learn library offers more than 100 types of  different models. To import the models to be used, you can use @sk_import. In this way, the following lines import respectively the first 3 models mentioned above that will be used in the practices of this subject:\n",
    "\n",
    "```Julia\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbours: KNeighborsClassifier\n",
    "```\n",
    "\n",
    "When training a model, the first step is to generate it. This is done with a different function for each model. This function receives as parameters the model's own parameters. Below are 3 examples, one for each type of model that will be used in these course assignments:\n",
    "\n",
    "```Julia\n",
    "model = SVC(kernel=\"rbf\", degree=3, gamma=2, C=1);\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=1);\n",
    "model = KNeighborsClassifier(3);\n",
    "```\n",
    "\n",
    "An explanation of the parameters accepted by each of these functions can be found in the library documentation. In the particular case of decision trees, as can be seen, one of these parameters is called `random_state`. This parameter controls the randomness in a particular part of the tree construction process, namely in the selection of features to split a node of the tree. The Scikit-Learn library uses a random number generator in this part, which is updated with each call, so that different calls to this function (together with its subsequent calls to the `fit!` function) to train the model will result in different models. To control the randomness of this process and make it deterministic, it is best to give it an integer value as shown in the example. Thus, the creation of a decision tree with a set of desired inputs and outputs and a given set of hyperparameters is a deterministic process. In general, it is more advisable to be able to control the randomness of the whole model development process (cross-validation, etc.) by means of a random seed that is set at the beginning of the whole process.\n",
    "\n",
    "Once created, any of these models can be adjusted with the `fit!` function.\n",
    "\n",
    "### Question\n",
    "\n",
    "What does the fact that the name of this function ends in bang (!) indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b8524",
   "metadata": {},
   "source": [
    "`By default, Julia passes the function parameters by value, not allowing them to be modified inside the function. Whenever we are calling a function with a ! we are indicating that the parameters passed are passed by reference, thus they can be modified inside the function. As we are calling fit! with a bang, we are assuming that the parameters we are passing to the fit function can be modified (i.e. the model after the fit! execution will be a trained model).`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5151f1",
   "metadata": {},
   "source": [
    "Contrary to the Flux library, where it was necessary to write the ANN training loop, in this library the loop is already implemented, and it is called automatically when the `fit!` function is executed. Therefore, it is not necessary to write the code for the training loop.\n",
    "\n",
    "### Question\n",
    "\n",
    "As in the case of ANNs, a loop is necessary for training several models. Where in the code (inside or outside the loop) will you need to create the model? Which models will need to be trained several times and which ones only once? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7437a0",
   "metadata": {},
   "source": [
    "`Only artificial neural networks models will need to be trained several times, as they depend on random seeds to perform the train. Support vector machines, decision trees or k-nn models are all mathematical deterministic models that no matter how many times they are trained they will always output the same result. If we desire to train our models several times the model must be created inside the loop.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc832c",
   "metadata": {},
   "source": [
    "An example of the use of this function can be seen in the following line:\n",
    "\n",
    "```Julia\n",
    "fit!(model, trainingInputs, trainingTargets);\n",
    "```\n",
    "\n",
    "As can be seen, the first argument of this function is the model, the second is an array of inputs, and the third is a vector of desired outputs. It is important to realise that this parameter with the desired outputs is not an array like in the case of ANNs but a vector whose each element will correspond to the label associated to that pattern, and can be of any type: integer, string, etc. The main reason for this is that there are some models that do not accept desired outputs with the one-hot-encoding.\n",
    "\n",
    "An important issue to consider is the layout of the data to be used. As has been shown in previous assignments, the patterns must be arranged in columns to train an ANN, being each row an attribute. Outside the world of ANNs, and therefore with the rest of the techniques to be used in this course, the patterns are usually assumed to be arranged in rows, and therefore each column in the input matrix corresponds to an attribute, being a much more intuitive way.\n",
    "\n",
    "### Question\n",
    "\n",
    "Which condition must the matrix of inputs and the vector of desired outputs passed as an argument to this function fulfil?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ed4cd",
   "metadata": {},
   "source": [
    "`As stated earlier, when training ANNs the input matrix will have a pattern per column and each row will be a feature of such pattern while for KNN, SVM and DT models the input matrix will have a pattern per row where each column will be a feature of such pattern. The output matrix will be a vector where the element i correspond to the expected output for the input pattern i. In the case of ANNs the vector must have numeric elements, while for KNN, SVM and DT can be any type. Suffice to say that both the input and output matrix will have to have the same number of patterns.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8087b",
   "metadata": {},
   "source": [
    "Finally, once the model has been trained, it can be used to make predictions. This is done by means of the predict function. An example of its use is shown below:\n",
    "\n",
    "```Julia\n",
    "testOutputs = predict(model, testInputs);\n",
    "```\n",
    "\n",
    "The model being used is an in-memory structure with different fields, and it can be very useful to look up the contents of these fields. To see which fields each model has, you can write the following:\n",
    "\n",
    "```Julia\n",
    "println(keys(model));\n",
    "```\n",
    "\n",
    "Depending on the type of model, there will be different fields. For example, for a kNN, the following fields, among others, could be consulted:\n",
    "\n",
    "```Julia\n",
    "model.n_neighbors\n",
    "model.metric\n",
    "model.weights\n",
    "```\n",
    "\n",
    "For an SVM, some other interesting fields could be the following:\n",
    "\n",
    "```Julia\n",
    "model.C\n",
    "model.support_vectors_\n",
    "model.support_\n",
    "model.support_\n",
    "```\n",
    "\n",
    "In the case of an SVM, a particularly interesting function is `decision_function`, which returns the distances to the hyperplane of the passed patterns. This is useful, for example, to implement a \"one-against-all\" strategy to perform multi-class classification. An example of the use of this function is shown below:\n",
    "\n",
    "```Julia\n",
    "distances = decision_function(model, inputs);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e501314",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "In the case of using decision trees or kNN, a corresponding function is not necessary to perform the \"one-against-all\" strategy, why?\n",
    "\n",
    "`The decision function returns an array where each member i indicates if the new input is to the 'left' or 'right' to the i hyperplane. As k-nearest neighbour algorithm does not need for hyperplanes, we wont need this function. `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca345045",
   "metadata": {},
   "source": [
    "However, the SVM implementation in the Scikit-Learn library already allows multi-class classification, so it is not necessary to use a \"one-against-all\" strategy for these cases.\n",
    "\n",
    "Finally, it should be noted that these models usually receive pre-processed inputs and outputs, with the most common pre-processing being the normalisation already described in a previous assignment. Therefore, the developed normalisation functions should also be used on the data to be used by these models.\n",
    "\n",
    "In this assignment, you are asked to develop a function called ```modelCrossValidation``` based on the functions developed in previous assignments that allows to validate models in the selected classification problem using the three techniques described here.\n",
    "\n",
    "This function should perform cross-validation and use the metrics deemed most appropriate for the specific problem. This cross-validation can be done by modifying the code developed in the previous assignment.\n",
    "\n",
    "This function must receive the following parameters:\n",
    "\n",
    "- Algorithm to be trained, among the 4 used in this course, together with its parameter. The most important parameters to specify for each technique are:\n",
    "    </br>\n",
    "    \n",
    "    - ANN\n",
    "        - Architecture (number of hidden layers and number of neurons in each hidden layer) and transfer funtion in each layer. In \"shallow\" networks such as those used in this course, the transfer function has less impact, so a standard one, shuch as `tansig` or `logsig`, can be used.\n",
    "        - Learning rate\n",
    "        - Ratio of patterns used for validation\n",
    "        - Number of consecutive iterations without improving the validation loss to stop the process\n",
    "        - Number of times each ANN is trained.\n",
    "        \n",
    "        ### Question\n",
    "        \n",
    "        Why should a linear transfer function not be used for neurons in the hidden layers?\n",
    "        \n",
    "        ```\n",
    "        In order for our model to be correctly trained artificial neural networks employ whats called the backpropagation algorithm. This algorithm adjusts the model weights in order to attempt to reduce the loss function. If we employ linear transfer functions in our model, then the backpropagation algorithm will not be able to know what connection weights adjust.```\n",
    "        \n",
    "        ### Question\n",
    "        \n",
    "        The other models do not have the number of times to train them as a parameter. Why? If you train several times, Which statistical properties will the results of these trainings have?\n",
    "        \n",
    "        ```Because the reason of training an ANN several times is the non-deterministic nature of ANNs. SVMs, KNNs or DTs are determinsitic, and, therefore, do not need to average the results obtained from repeated trainings.```\n",
    "        \n",
    "        \n",
    "    </br>  \n",
    "    \n",
    "    - SVM\n",
    "        - Kernel (and kernel-specific parameters)\n",
    "        - C\n",
    "        \n",
    "    - Decision trees\n",
    "        - Maximum tree depth\n",
    "        \n",
    "    - kNN\n",
    "        - k (number of neighbours to be considered)\n",
    "        \n",
    "        \n",
    "   </br> \n",
    "- Already standardised input and desired outputs matrices.\n",
    "    </br>  \n",
    "    - As stated above, the desired outputs must be indicated as a vector where each element is the label corresponding to each pattern (therefore, of type `Array{Any,1}`). In the case of ANN training, the desired outputs shall be encoded as done in previous assignments.\n",
    "    </br>  \n",
    "    - As previously described, in the case of using techniques such as SVM, decision trees or kNN, the one-hot-encoding configuration will not be used. In these cases, the `confusionMatrix` function developed in a previous assignment will be used to calculate the metrics, which accepts as input two vectors (outputs and desired outputs) of type `Array{Any,1}`.\n",
    "    \n",
    "    ### Question\n",
    "    \n",
    "    Has it been necessary to standardise the desired outputs? Why?\n",
    "    \n",
    "    ```We do not need to standarise the outputs because the classifiers we are employing are insensitive to the scale of the outputs.```\n",
    "\n",
    "    </br> \n",
    "- Cross-validation indices. It is important to note that, as in the previous assignment, the partitioning of the patterns in each fold need to be done outside this function, because this allows this same partitioning to be used then training other models. In this way, cross-validation is performed with the same data and the same partitions in all classes.\n",
    "\n",
    "Since most of the code will be the same, do not develop 4 different functions, one for each model, but only one function. Inside it, at the time of generation the model in each fold, and depending on the model, the following changes should be made:\n",
    "\n",
    "- If the model is an ANN, the desired outputs shall be encoded by means of the code developed in previous assignments. As this model is non-deterministic, it will be nevessary to make a new loop to train several ANNs, splitting the training data into training and validation (if validation set is used) and calling the function defined in previous assignments to create and traing an ANN.\n",
    "\n",
    "- If the model is not an ANN, the code that trains the model shall be developed. This code shall be the same for each of the rematining 3 types of models (SVM, decision trees, and KNN), with the line where the model is called being the only difference.\n",
    "\n",
    "In turn, this function should return, at least, the values for the selected metrics. Once this function has been developed, the experimental part of the assignment begins. The objective is to determine which model with a specific combination of hyperparameters offers the best results, for which the above function will be run for each of the 4 types of models, and for each model it will be run with different values in its hyperparameters.\n",
    "\n",
    "- The results obtained should be documented in the report to be produced, for which it will be useful to show the results in tabular and/or graphical form.\n",
    "\n",
    "- When it comes to displaying a confusion matrix in the report, an important question is which one to show given that a lot of trainings have been performed. The cross-validation technique does not generate a final model, but allows comparing different algorithms and configurations to choose the model or parameter configuration that returns the best results. Once chosen, it is necessary to train a \"final\" model from scratch by using all the patterns as the training set, that is, without separating patterns for testing. In this way, the performance of this model and configuration is expected to be slightly higher than that obtained through cross-validation, since more patterns have been used to train it. This is the final model that would be used in production, and from which a confusion matrix can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff85088d-0b99-4e15-bfe1-bc0e0bd6b044",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Running `conda install -y -c conda-forge 'libstdcxx-ng>=3.4,<11.4'` in root environment\n",
      "└ @ Conda /home/poli/.julia/packages/Conda/x2UxR/src/Conda.jl:127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Running `conda install -y -c conda-forge 'libstdcxx-ng>=3.4,<11.4'` in root environment\n",
      "└ @ Conda /home/poli/.julia/packages/Conda/x2UxR/src/Conda.jl:127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Running `conda install -y -c conda-forge 'libstdcxx-ng>=3.4,<11.4'` in root environment\n",
      "└ @ Conda /home/poli/.julia/packages/Conda/x2UxR/src/Conda.jl:127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.neighbors._classification.KNeighborsClassifier'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbors: KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b7e4168-4642-4d6f-b6ce-6455b77333ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol\n",
      "PyCall.PyObject\n"
     ]
    }
   ],
   "source": [
    "# Test of the symbol type\n",
    "# as we can see if we employ eval(symbol) \n",
    "# we get the reference\n",
    "\n",
    "println(typeof(:SVC))\n",
    "println(typeof(eval(:SVC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "130f2d0a-62b8-4919-b69f-0d4925d20e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode_categories (generic function with 1 method)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Dates;\n",
    "using Statistics;\n",
    "using Random;\n",
    "\n",
    "#############################################################\n",
    "############# FUNCTIONS FROM PREVIOUS NOTEBOOKS #############\n",
    "#############  (crossvalidation functions were  #############\n",
    "#############    remade to work as expected)    #############\n",
    "#############################################################\n",
    "\n",
    "####### ANN RELATED\n",
    "####### FUNCTIONS\n",
    "\n",
    "function holdOut(N::Int, P::Real)    \n",
    "    # generate random index vector\n",
    "    index_vector=Random.randperm(MersenneTwister(Dates.datetime2epochms(Dates.now())), N)\n",
    "    cut_point = floor(Int,N*P)\n",
    "    cut_set = index_vector[1:cut_point]\n",
    "    train_set = index_vector[cut_point:length(index_vector)]\n",
    "    return train_set, cut_set\n",
    "end\n",
    "\n",
    "function buildClassANN(numInputs::Int, topology::AbstractArray{<:Int,1}, numOutputs::Int;\n",
    "                    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology))) \n",
    "    ann = Chain()\n",
    "    numInputsLayer = numInputs\n",
    "    for numOutputLayers = topology\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, numOutputLayers, σ))\n",
    "        numInputsLayer = numOutputLayers\n",
    "    end\n",
    "    if (numOutputs == 1)\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, 1, σ))\n",
    "    else\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, numOutputs, identity))\n",
    "        ann = Chain(ann..., softmax)\n",
    "    end\n",
    "    return ann\n",
    "end\n",
    "\n",
    "# Function to train classification artificial neural networks\n",
    "function trainClassANN(\n",
    "        topology::AbstractArray{<:Int,1},  \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}; \n",
    "        validationDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}= \n",
    "                    (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0,0)), \n",
    "        testDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}= \n",
    "                    (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0,0)), \n",
    "        transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)), \n",
    "        maxEpochs::Int=1000, \n",
    "        minLoss::Real=0.0, \n",
    "        learningRate::Real=0.01,  \n",
    "        maxEpochsVal::Int=20, \n",
    "        showText::Bool=false)\n",
    "    \n",
    "    # Create ANN and loss function for classification problems\n",
    "    training_inputs, training_outputs = trainingDataset\n",
    "    validation_inputs, validation_outputs = validationDataset\n",
    "    test_inputs, test_outputs = testDataset\n",
    "    \n",
    "    input_feats_size, output_classes_size = size(training_inputs,2), size(training_outputs,2)\n",
    "    \n",
    "    ann = buildClassANN(input_feats_size, topology, output_classes_size)\n",
    "    loss(x, y) = (size(y,1) == 1) ? Losses.binarycrossentropy(ann(x),y) : Losses.crossentropy(ann(x),y)\n",
    "    \n",
    "    # Compute base array\n",
    "    training_losses = Float64[]\n",
    "    validation_losses = Float64[]\n",
    "    test_losses = Float64[]\n",
    "    \n",
    "    training_accuracies = Float64[]\n",
    "    validation_accuracies = Float64[]\n",
    "    test_accuracies = Float64[]\n",
    "\n",
    "    # Metrics computation inner function\n",
    "    \n",
    "    current_epoch = 0  \n",
    "    current_epoch_val = 0\n",
    "\n",
    "    function compute_metrics()\n",
    "        training_loss = loss(training_inputs', training_outputs')\n",
    "        validation_loss = loss(validation_inputs', validation_outputs')\n",
    "        test_loss = loss(test_inputs', test_outputs')\n",
    "        \n",
    "        training_ann_outputs = ann(training_inputs')\n",
    "        validation_ann_outputs = ann(validation_inputs')\n",
    "        test_ann_outputs = ann(test_inputs')\n",
    "        \n",
    "        training_accuracy = accuracy(training_ann_outputs', training_outputs)\n",
    "        validation_accuracy = accuracy(validation_ann_outputs', validation_outputs)\n",
    "        test_accuracy = accuracy(test_ann_outputs', test_outputs)\n",
    "\n",
    "        if showText\n",
    "            println(\"Epoch \", current_epoch)\n",
    "            println(\"Training loss: \", training_loss, \", Training accuracy: \", 100*training_accuracy,\" %\")\n",
    "            if length(validation_inputs) > 1\n",
    "                println(\"Validation loss: \", validation_loss, \", Validation accuracy: \", 100*validation_accuracy, \" %\") \n",
    "            end\n",
    "            if length(test_inputs) > 1\n",
    "                println(\"Test loss: \", test_loss, \", Test accuracy: \", 100*test_accuracy,\" %\")\n",
    "            end\n",
    "        end\n",
    "        return (training_loss, training_accuracy, validation_loss, \n",
    "            validation_accuracy, test_loss, test_accuracy)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    # Compute initial metrics\n",
    "    (training_loss, training_accuracy, validation_loss, \n",
    "        validation_accuracy, test_loss, test_accuracy) = compute_metrics()\n",
    "    \n",
    "    push!(training_losses, training_loss)\n",
    "    push!(validation_losses, validation_loss)\n",
    "    push!(test_losses, test_loss)\n",
    "    \n",
    "    push!(training_accuracies, training_accuracy)\n",
    "    push!(validation_accuracies, validation_accuracy)\n",
    "    push!(test_accuracies, test_accuracy)\n",
    "    \n",
    "        \n",
    "    # Store initial ANN as the 'best'\n",
    "    best_validation_loss = validation_loss;\n",
    "    final_ann = deepcopy(ann);\n",
    "    \n",
    "    # Training loop\n",
    "    while (current_epoch < maxEpochs) && (training_loss > minLoss) && (current_epoch_val < maxEpochsVal)\n",
    "        current_epoch += 1\n",
    "        \n",
    "        Flux.train!(loss, Flux.params(ann), [(training_inputs', training_outputs')], ADAM(learningRate))\n",
    "        (training_loss, training_accuracy, validation_loss, \n",
    "            validation_accuracy, test_loss, test_accuracy) = compute_metrics();\n",
    "        \n",
    "        push!(training_losses, training_loss)\n",
    "        push!(validation_losses, validation_loss)\n",
    "        push!(test_losses, test_loss)\n",
    "        \n",
    "        # Check for early stop (only if we have a validation dataset)\n",
    "        if length(validation_inputs) > 1\n",
    "            if (validation_loss < best_validation_loss)\n",
    "                # reset the number of validation epochs, because we have an improved metric\n",
    "                # and store current ann as best\n",
    "                if showText\n",
    "                    println(\"[->] Found new best model: old_val_loss=\",validation_loss,\", new_val_loss=\",best_validation_loss)\n",
    "                end\n",
    "                current_epoch_val = 0;\n",
    "                best_validation_loss = validation_loss;\n",
    "                final_ann = deepcopy(ann);\n",
    "            else\n",
    "                current_epoch_val += 1;\n",
    "            end\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return (final_ann, training_losses, validation_losses, test_losses, training_accuracies, validation_accuracies, test_accuracies)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function trainClassANN(\n",
    "        topology::AbstractArray{<:Int,1}, \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}, \n",
    "        kFoldIndices::\tArray{Int64,1}; \n",
    "        transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)), \n",
    "        maxEpochs::Int=1000, \n",
    "        minLoss::Real=0.0, \n",
    "        learningRate::Real=0.01, \n",
    "        repetitionsTraining::Int=1, \n",
    "        validationRatio::Real=0.0, \n",
    "        maxEpochsVal::Int=20)\n",
    "\n",
    "    \n",
    "    # Transform the kFoldIndexes into a set to remove duplicate and\n",
    "    # compute the number of folds as the size of that set\n",
    "    numFolds = size(unique(kFoldIndices))[1]\n",
    "    \n",
    "    # create our metrics vectors\n",
    "    train_losses_array = []\n",
    "    train_accuracy_array = []\n",
    "    \n",
    "    test_losses_array  = []\n",
    "    test_accuracy_array = []\n",
    "    \n",
    "    validation_losses_array = []\n",
    "    validation_accuracy_array = []\n",
    "        \n",
    "    for numFold = 1:numFolds\n",
    "        # Extract the train and test dataset using the k-folds\n",
    "        train_in  = trainingDataset[1][kFoldIndices.!=numFold,:]\n",
    "        train_out = trainingDataset[2][kFoldIndices.!=numFold,:]\n",
    "        \n",
    "        test_in   = trainingDataset[1][kFoldIndices.==numFold,:]\n",
    "        test_out  = trainingDataset[2][kFoldIndices.==numFold,:]\n",
    "        \n",
    "        # If we are using a validation dataset, perform a holdout\n",
    "        train_idx, val_idx = holdOut(size(train_in,1), validationRatio)\n",
    "            \n",
    "        train_dataset      = train_in[train_idx,:], train_out[train_idx,:]\n",
    "        validation_dataset = train_in[val_idx,:], train_out[val_idx,:]\n",
    "        test_dataset       = test_in, test_out\n",
    "\n",
    "        for i=1:repetitionsTraining\n",
    "            (final_ann, train_losses, validation_losses, test_losses, training_accuracies, \n",
    "                validation_accuracies, test_accuracies) = trainClassANN(topology, \n",
    "                                                                        train_dataset, \n",
    "                                                                        validationDataset=validation_dataset, \n",
    "                                                                        testDataset=test_dataset, \n",
    "                                                                        transferFunctions=transferFunctions, \n",
    "                                                                        maxEpochs=maxEpochs, \n",
    "                                                                        minLoss=minLoss, \n",
    "                                                                        learningRate=learningRate, \n",
    "                                                                        maxEpochsVal=maxEpochsVal,\n",
    "                                                                        showText=true)\n",
    "\n",
    "            # Append the data to our arrays\n",
    "            append!(train_losses_array, train_losses)\n",
    "            append!(train_accuracy_array, training_accuracies)\n",
    "\n",
    "            append!(test_losses_array, test_losses)\n",
    "            append!(test_accuracy_array, test_accuracies)\n",
    "\n",
    "            append!(validation_losses_array, validation_losses)\n",
    "            append!(validation_accuracy_array, validation_accuracies)\n",
    "        end\n",
    "    end\n",
    "                \n",
    "    return (train_losses_array, train_accuracy_array, test_losses_array, test_accuracy_array,\n",
    "        validation_losses_array, validation_accuracy_array)\n",
    "end\n",
    "\n",
    "\n",
    "####### CONFUSION MATRIX \n",
    "####### RELATED FUNCTIONS\n",
    "\n",
    "\n",
    "\n",
    "function accuracy(outputs::AbstractArray{Bool,2}, targets::AbstractArray{Bool,2}) \n",
    "\n",
    "    if (size(targets,2)==1)\n",
    "        return accuracy(outputs[:,1], targets[:,1])\n",
    "    else\n",
    "        classComparison = targets .== outputs\n",
    "        correctClassifications = all(classComparison, dims=2)\n",
    "        return mean(correctClassifications)\n",
    "    end\n",
    "end\n",
    "\n",
    "function accuracy(outputs::AbstractArray{<:Real,2}, targets::AbstractArray{Bool,2}, threshold::Real=0.5)\n",
    "    if (size(targets,2)==1)\n",
    "        return accuracy(outputs[:,1], targets[:,1])\n",
    "    else\n",
    "        classified_outputs=classifyOutputs(outputs)\n",
    "        return accuracy(classified_outputs, targets)\n",
    "    end\n",
    "end\n",
    "\n",
    "function classifyOutputs(outputs::AbstractArray{<:Real,2}; \n",
    "                        threshold::Real=0.5)\n",
    "    if size(outputs, 2) == 1\n",
    "        output = dataset .>= threshold\n",
    "    else\n",
    "        (_,indicesMaxEachInstance) = findmax(outputs, dims=2);\n",
    "        bool_outputs = falses(size(outputs));\n",
    "        bool_outputs[indicesMaxEachInstance] .= true\n",
    "    end\n",
    "    return bool_outputs\n",
    "end\n",
    "\n",
    "function confusionMatrix(outputs::AbstractArray{Bool,1}, targets::AbstractArray{Bool,1})\n",
    "    \n",
    "    tp = sum(outputs .& targets)      # select all true outputs that are true on target\n",
    "    fp = sum(outputs .& .!targets)    # select all true outputs that are false on target\n",
    "    tn = sum(.!outputs .& .!targets)  # select all false outputs that are false on target\n",
    "    fn = sum(.!outputs .& targets)    # select all false outputs that are true on target\n",
    "    \n",
    "    conf_matrix = [tn fp; fn tp]\n",
    "    \n",
    "    accu= (tn+tp)/(tn+tp+fn+fp)\n",
    "    erra= (fp+fn)/(tn+tp+fn+fp)\n",
    "    reca= (tn==length(targets)) ? (tp/(fn+tp)) : 1\n",
    "    spec= (tp==length(targets)) ? (tn/(fp+tn)) : 1\n",
    "    prec= (tn==length(targets)) ? (tp/(tp+fp)) : 1\n",
    "    npre= (tp==length(targets)) ? (tn/(tn+fn)) : 1\n",
    "    \n",
    "    f1 = (reca==prec==0) ? 2*(prec*reca/prec+reca) : 0\n",
    "    \n",
    "    return accu,erra,reca,spec,prec,npre,f1,conf_matrix\n",
    "end\n",
    "\n",
    "function confusionMatrix(outputs::AbstractArray{<:Real,1},targets::AbstractArray{Bool,1}; threshold::Real=0.5)\n",
    "    outputs_boolean = outputs .> threshold\n",
    "    return confusionMatrix(outputs_boolean, targets)\n",
    "end\n",
    "\n",
    "\n",
    "#########  ONE HOT ENCODING\n",
    "#########  RELATED FUNCTIONS\n",
    "function oneHotEncoding(feature::AbstractArray{<:Any,1}, classes::AbstractArray{<:Any,1})\n",
    "    numClasses = length(unique(classes))\n",
    "\n",
    "    if (numClasses == 2)\n",
    "        oneHot = Array{Bool,2}(undef, size(feature,1), 1)\n",
    "        oneHot[:,1] .= (feature.==classes[1])\n",
    "    else\n",
    "        oneHot = Array{Bool,2}(undef, size(feature,1), numClasses)\n",
    "        for numClass = 1:numClasses\n",
    "            oneHot[:,numClass] .= (feature.==classes[numClass])\n",
    "        end\n",
    "    end\n",
    "    return oneHot\n",
    "end\n",
    "function oneHotEncoding(feature::AbstractArray{<:Any,1})\n",
    "    return oneHotEncoding(feature, unique(feature))\n",
    "end\n",
    "\n",
    "#########  CROSSVALIDATION\n",
    "#########  FUNCTIONS\n",
    "\n",
    "\n",
    "## default k-fold\n",
    "function crossvalidation(N::Int64, k::Int64)\n",
    "    indices = repeat(1:k, Int64(ceil(N/k)))\n",
    "    indices = indices[1:N]\n",
    "    shuffle!(indices)\n",
    "    return indices\n",
    "end;\n",
    "\n",
    "## k-fold with balanced k-sets\n",
    "function crossvalidation(targets::AbstractArray{Bool,2}, k::Int64)\n",
    "    # compute the nubmer of elements in our targets dataset\n",
    "    n_rows = size(targets,1)\n",
    "    \n",
    "    indexes = zeros(Int64, size(targets,1))\n",
    "    for class in eachcol(targets)\n",
    "        n_elements = sum(class)\n",
    "        current_class_indexes = crossvalidation(n_elements, k)\n",
    "        \n",
    "        i = 1\n",
    "        j = 1\n",
    "        for element in class\n",
    "            if element == true \n",
    "                indexes[i] = current_class_indexes[j]\n",
    "                j+=1\n",
    "            end\n",
    "            i+=1\n",
    "        end\n",
    "    end\n",
    "    return indexes;\n",
    "end\n",
    "\n",
    "## k-fold with balanced k-sets and one-hot wrapper\n",
    "function crossvalidation(targets::AbstractArray{<:Any,1}, k::Int64)\n",
    "    targets = oneHotEncoding(targets, unique(targets));\n",
    "    crossValidationIndices = crossvalidation(size(targets,1), k);\n",
    "    \n",
    "    return crossValidationIndices;\n",
    "end;\n",
    "\n",
    "#########  NORMALISATION\n",
    "#########  FUNCTIONS\n",
    "\n",
    "function stats(outputs)\n",
    "    minimum = mapslices(Statistics.minimum, outputs; dims=1)[1]\n",
    "    maximum = mapslices(Statistics.maximum, outputs; dims=1)[1]\n",
    "    mean = mapslices(Statistics.mean, outputs; dims=1)[1]\n",
    "    std = mapslices(Statistics.std, outputs; dims=1)[1]\n",
    "    return [minimum, maximum, mean, std]\n",
    "end\n",
    "\n",
    "function calculateMinMaxNormalizationParameters(dataset::AbstractArray{<:Real,2})\n",
    "    # function that takes a real matrix (i.e. array of reals with dimension 2)\n",
    "    # this matrix is the data-set to our problem, where each row is a sample and each column is an attribute\n",
    "    # return a 2-tuple of matrixes where each row is the minimum and maximum respectivelly\n",
    "    \n",
    "    min_matrix = []\n",
    "    max_matrix = []\n",
    "    \n",
    "    for column in eachcol(dataset)\n",
    "        r = stats(column)\n",
    "        if min_matrix == [] || max_matrix == []\n",
    "            min_matrix = r[1]\n",
    "            max_matrix = r[2]\n",
    "        else\n",
    "            min_matrix = vcat(min_matrix, r[1])\n",
    "            max_matrix = hcat(max_matrix, r[2])\n",
    "        end\n",
    "    end\n",
    "    return reshape(min_matrix, (4,1)), reshape(max_matrix, (4,1))\n",
    "end\n",
    "\n",
    "function normalizeMinMax( dataset::AbstractArray{<:Real,2})\n",
    "    # x scaled = x - min(x) / max(x) - min(x)\n",
    "    min, max = calculateMinMaxNormalizationParameters(dataset)\n",
    "    out = zeros(size(dataset, 1), size(dataset, 2))\n",
    "    for i in axes(dataset, 1)\n",
    "        for j in axes(dataset, 2)\n",
    "            cmin, cmax = min[j], max[j]\n",
    "            out[i,j] = dataset[i,j] - cmin / (cmax - cmin)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return out\n",
    "end\n",
    "function normalizeZeroMean( dataset::AbstractArray{<:Real,2}) \n",
    "    mean, std = calculateZeroMeanNormalizationParameters(dataset)\n",
    "    out = zeros(size(dataset, 1), size(dataset, 2))\n",
    "    for i in axes(dataset, 1)\n",
    "        for j in axes(dataset, 2)\n",
    "            cmean, cstd = mean[j], std[j]\n",
    "            out[i,j] = dataset[i,j] - cmean / cstd\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function encode_categories(targets)\n",
    "    if (length(unique(targets)) > 2)\n",
    "        cats = unique(targets) .== permutedims(targets)\n",
    "        return cats'\n",
    "    else\n",
    "        cats = targets .== unique(targets)[1]\n",
    "        return cats\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "17e6c492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelCrossValidation (generic function with 1 method)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function modelCrossValidation(modelType::Symbol,\n",
    "        modelHyperparameters::Dict,\n",
    "        inputs::AbstractArray{<:Real,2},\n",
    "        targets::AbstractArray{<:Any,1},\n",
    "        crossValidationIndices::Array{Int64,1})\n",
    "    \n",
    "    # Extract data from inputs and targets\n",
    "    n_inputs  = size(inputs, 1)\n",
    "    n_feats   = size(inputs, 2)\n",
    "    n_classes = size(targets,2)\n",
    "    \n",
    "    \n",
    "    # compute crossvalidation data\n",
    "    \n",
    "    # Transform the kFoldIndexes into a set to remove duplicate and\n",
    "    # compute the number of folds as the size of that set\n",
    "    numFolds = size(unique(crossValidationIndices),1)\n",
    "    \n",
    "    # create our metrics vectors\n",
    "    # as we will not do a validation hold-out we will not create validation_xxx_arrays\n",
    "    train_losses_array = []\n",
    "    test_losses_array  = []\n",
    "    train_accuracies_array = []\n",
    "    test_accuracies_array = []\n",
    "    \n",
    "    # Build the model\n",
    "    \n",
    "    if modelType == :ANN\n",
    "        println(\"Artificial Neural Network\")\n",
    "        \n",
    "        targets = oneHotEncoding(targets)\n",
    "        \n",
    "        # get hyperparameters\n",
    "        architecture = modelHyperparameters[\"architecture\"]\n",
    "        lr           = modelHyperparameters[\"lr\"]\n",
    "        val_ratio    = modelHyperparameters[\"val_ratio\"]\n",
    "        epochs       = modelHyperparameters[\"epochs\"]\n",
    "        early_stop   = modelHyperparameters[\"early_stop_epochs\"]\n",
    "        n_train      = modelHyperparameters[\"n_train\"]\n",
    "        # as we have already developed a crossvalidation function for\n",
    "        # flux-built anns we will just return it here\n",
    "        outputs = \n",
    "        (train_losses_array, train_accuracy_array, test_losses_array, test_accuracy_array,\n",
    "        validation_losses_array, validation_accuracy_array) = trainClassANN(\n",
    "            architecture, \n",
    "            (inputs, targets),\n",
    "            crossValidationIndices, \n",
    "            validationRatio=val_ratio, \n",
    "            learningRate=lr, \n",
    "            maxEpochsVal=early_stop, \n",
    "            repetitionsTraining=n_train, \n",
    "            maxEpochs=epochs)\n",
    "\n",
    "        return (train_losses_array, train_accuracy_array, test_losses_array, test_accuracy_array,\n",
    "        validation_losses_array, validation_accuracy_array)\n",
    "        \n",
    "        \n",
    "    elseif modelType == :SVC\n",
    "        println(\"Support Vector Machine\")\n",
    "        \n",
    "        kernel = modelHyperparameters[\"kernel\"]\n",
    "        pol_degree = modelHyperparameters[\"degree\"]\n",
    "        gamma = modelHyperparameters[\"gamma\"]\n",
    "        c_val = modelHyperparameters[\"c\"]\n",
    "        \n",
    "        model = SVC(kernel=kernel, degree=pol_degree, gamma=gamma, C=c_val);\n",
    "\n",
    "    elseif modelType == :DecisionTreeClassifier\n",
    "        println(\"Decision Tree\")\n",
    "        \n",
    "        max_depth = modelHyperparameters[\"max_depth\"]\n",
    "        \n",
    "        model = DecisionTreeClassifier(max_depth=max_depth)\n",
    "            \n",
    "    elseif modelType == :KNeighborsClassifier\n",
    "        print(\"K-Nearest Neighbors\")\n",
    "            \n",
    "        n_neighbours = modelHyperparameters[\"n_neighbours\"]\n",
    "        \n",
    "        model = KNeighborsClassifier(n_neighbours);\n",
    "    else\n",
    "        println(\"Unknown model type\")\n",
    "    end\n",
    "    \n",
    "    # now the model is built, perform training\n",
    "    # this is performed the same way as with crossvalidation ANN\n",
    "    \n",
    "    test_accuracies = []\n",
    "    classes = unique(targets)\n",
    "    for numFold = 1:numFolds\n",
    "        # Extract the train and test dataset using the k-folds\n",
    "        train_input  = inputs[crossValidationIndices.!=numFold,:]\n",
    "        train_target = targets[crossValidationIndices.!=numFold,:][:,1]\n",
    "        \n",
    "        test_input  = inputs[crossValidationIndices.==numFold,:]\n",
    "        test_target  = targets[crossValidationIndices.==numFold,:][:,1]\n",
    "        \n",
    "        fit!(model, train_input, vec(train_target))\n",
    "        test_outputs=predict(model, test_input)\n",
    "\n",
    "        \n",
    "        onehot_outputs = oneHotEncoding(test_outputs, classes)\n",
    "        onehot_targets = oneHotEncoding(test_target, classes)\n",
    "        append!(test_accuracies,accuracy(onehot_outputs, onehot_targets))\n",
    "    end\n",
    "    return test_accuracies\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3867d4c-21b4-4477-805c-cafb8418a6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "164930ae-d210-4cb8-8063-ec2070eca9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":Done"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DelimitedFiles \n",
    "\n",
    "\n",
    "#### Read the data\n",
    "dataset = readdlm(\"iris.data\",',');\n",
    "\n",
    "inputs = dataset[:,1:4];\n",
    "inputs = convert(Array{Float32,2}, inputs); \n",
    "norm_input = normalizeMinMax(inputs)\n",
    "targets = dataset[:,5];\n",
    "\n",
    "#### Create the k-folds\n",
    "k = 10\n",
    "cross_val_indexes = crossvalidation(targets, k)\n",
    "\n",
    ":Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ade5e341-7580-4154-9e20-fbe14b9e80b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " 1.0\n",
       " 0.8666666666666667\n",
       " 1.0\n",
       " 0.8666666666666667\n",
       " 1.0\n",
       " 0.9333333333333333\n",
       " 0.9333333333333333\n",
       " 0.9333333333333333\n",
       " 0.9333333333333333\n",
       " 1.0"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = :KNeighborsClassifier\n",
    "\n",
    "hyperparameters                    = Dict()\n",
    "\n",
    "hyperparameters[\"n_neighbours\"]    = 2\n",
    "\n",
    "modelCrossValidation(symbol, hyperparameters, inputs, targets, cross_val_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "23fc9321-dd0d-4e33-8c8a-4dece857b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " 1.0\n",
       " 0.8\n",
       " 1.0\n",
       " 0.8\n",
       " 1.0\n",
       " 0.8666666666666667\n",
       " 0.9333333333333333\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = :SVC\n",
    "\n",
    "hyperparameters              = Dict()\n",
    "\n",
    "hyperparameters[\"kernel\"]    = \"poly\"\n",
    "hyperparameters[\"degree\"]    = 3\n",
    "hyperparameters[\"gamma\"]     = \"auto\"\n",
    "hyperparameters[\"c\"]         = 10\n",
    "\n",
    "modelCrossValidation(symbol, hyperparameters, inputs, targets, cross_val_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "183702aa-d439-4bc8-bca8-9dbdbcc62cae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1 2 3], Bool[1;;])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1 2 3; \n",
    "    4 5 6]\n",
    "b = [true; false]\n",
    "s = (a,b)\n",
    "\n",
    "print(a[b,:]);\n",
    "s[1][b,:], s[2][b,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1a28c9f5-cbff-43e8-a4fb-0d62dfb595d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Neural NetworkEpoch 0\n",
      "Training loss: 1.1058108, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1474886, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0411938, Test accuracy: 40.0 %\n",
      "Epoch 1\n",
      "Training loss: 1.104888, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1462607, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0421405, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1462607, new_val_loss=1.1474886\n",
      "Epoch 2\n",
      "Training loss: 1.1039745, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1450423, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0430951, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1450423, new_val_loss=1.1462607\n",
      "Epoch 3\n",
      "Training loss: 1.1030706, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.143833, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0440577, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.143833, new_val_loss=1.1450423\n",
      "Epoch 4\n",
      "Training loss: 1.1021769, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1426332, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0450287, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1426332, new_val_loss=1.143833\n",
      "Epoch 5\n",
      "Training loss: 1.1012932, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1414428, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0460078, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1414428, new_val_loss=1.1426332\n",
      "Epoch 6\n",
      "Training loss: 1.1004187, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1402618, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0469954, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1402618, new_val_loss=1.1414428\n",
      "Epoch 7\n",
      "Training loss: 1.099555, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1390904, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0479912, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1390904, new_val_loss=1.1402618\n",
      "Epoch 8\n",
      "Training loss: 1.0987004, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1378483, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0487676, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1378483, new_val_loss=1.1390904\n",
      "Epoch 9\n",
      "Training loss: 1.0978557, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1366155, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0495515, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1366155, new_val_loss=1.1378483\n",
      "Epoch 10\n",
      "Training loss: 1.0970206, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.1353917, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.0503433, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.1353917, new_val_loss=1.1366155\n",
      "Epoch 0\n",
      "Training loss: 1.127547, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1171713, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.285704, Test accuracy: 33.33333333333333 %\n",
      "Epoch 1\n",
      "Training loss: 1.1263475, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1161929, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2837577, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1161929, new_val_loss=1.1171713\n",
      "Epoch 2\n",
      "Training loss: 1.1251577, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1152246, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2818222, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1152246, new_val_loss=1.1161929\n",
      "Epoch 3\n",
      "Training loss: 1.1239773, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1142663, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2798977, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1142663, new_val_loss=1.1152246\n",
      "Epoch 4\n",
      "Training loss: 1.1228065, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1133183, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.277984, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1133183, new_val_loss=1.1142663\n",
      "Epoch 5\n",
      "Training loss: 1.121646, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1123803, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2760813, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1123803, new_val_loss=1.1133183\n",
      "Epoch 6\n",
      "Training loss: 1.1204948, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1114526, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2741896, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1114526, new_val_loss=1.1123803\n",
      "Epoch 7\n",
      "Training loss: 1.1193539, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1105355, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.272309, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1105355, new_val_loss=1.1114526\n",
      "Epoch 8\n",
      "Training loss: 1.1182228, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1096287, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2704396, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1096287, new_val_loss=1.1105355\n",
      "Epoch 9\n",
      "Training loss: 1.1171018, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1087322, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2685814, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1087322, new_val_loss=1.1096287\n",
      "Epoch 10\n",
      "Training loss: 1.1159909, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.1078464, Validation accuracy: 38.46153846153847 %\n",
      "Test loss: 1.2667339, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.1078464, new_val_loss=1.1087322\n",
      "Epoch 0\n",
      "Training loss: 1.1282009, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0959067, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0837805, Test accuracy: 40.0 %\n",
      "Epoch 1\n",
      "Training loss: 1.1274624, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0958225, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0835904, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0958225, new_val_loss=1.0959067\n",
      "Epoch 2\n",
      "Training loss: 1.1267316, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0957453, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0834072, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0957453, new_val_loss=1.0958225\n",
      "Epoch 3\n",
      "Training loss: 1.1260091, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0956758, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0832317, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0956758, new_val_loss=1.0957453\n",
      "Epoch 4\n",
      "Training loss: 1.125294, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0956135, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0830632, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0956135, new_val_loss=1.0956758\n",
      "Epoch 5\n",
      "Training loss: 1.1245872, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0955584, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0829024, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0955584, new_val_loss=1.0956135\n",
      "Epoch 6\n",
      "Training loss: 1.1238881, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0955107, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0827487, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0955107, new_val_loss=1.0955584\n",
      "Epoch 7\n",
      "Training loss: 1.1231967, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0954703, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0826021, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0954703, new_val_loss=1.0955107\n",
      "Epoch 8\n",
      "Training loss: 1.1225132, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0954585, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0824635, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0954585, new_val_loss=1.0954703\n",
      "Epoch 9\n",
      "Training loss: 1.1218375, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0954545, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0823323, Test accuracy: 40.0 %\n",
      "[->] Found new best model: old_val_loss=1.0954545, new_val_loss=1.0954585\n",
      "Epoch 10\n",
      "Training loss: 1.1211693, Training accuracy: 32.52032520325203 %\n",
      "Validation loss: 1.0954578, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0822083, Test accuracy: 40.0 %\n",
      "Epoch 0\n",
      "Training loss: 1.2394569, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.415503, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2516656, Test accuracy: 33.33333333333333 %\n",
      "Epoch 1\n",
      "Training loss: 1.237645, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.4125862, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2498404, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.4125862, new_val_loss=1.415503\n",
      "Epoch 2\n",
      "Training loss: 1.2358408, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.4096794, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2480245, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.4096794, new_val_loss=1.4125862\n",
      "Epoch 3\n",
      "Training loss: 1.2340451, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.406782, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2462173, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.406782, new_val_loss=1.4096794\n",
      "Epoch 4\n",
      "Training loss: 1.2322576, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.4038944, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.244419, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.4038944, new_val_loss=1.406782\n",
      "Epoch 5\n",
      "Training loss: 1.2304788, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.4010165, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2426296, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.4010165, new_val_loss=1.4038944\n",
      "Epoch 6\n",
      "Training loss: 1.2287081, Training accuracy: 34.146341463414636 %\n",
      "Validation loss: 1.3981484, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2408494, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.3981484, new_val_loss=1.4010165\n",
      "Epoch 7\n",
      "Training loss: 1.2269462, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.3952903, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.239078, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.3952903, new_val_loss=1.3981484\n",
      "Epoch 8\n",
      "Training loss: 1.2251929, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.3924419, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2373157, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.3924419, new_val_loss=1.3952903\n",
      "Epoch 9\n",
      "Training loss: 1.2234476, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.3896035, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2355627, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.3896035, new_val_loss=1.3924419\n",
      "Epoch 10\n",
      "Training loss: 1.221711, Training accuracy: 33.33333333333333 %\n",
      "Validation loss: 1.3867749, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.2338185, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.3867749, new_val_loss=1.3896035\n",
      "Epoch 0\n",
      "Training loss: 1.132321, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1433442, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0255964, Test accuracy: 53.333333333333336 %\n",
      "Epoch 1\n",
      "Training loss: 1.1311651, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1422396, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0261791, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.1422396, new_val_loss=1.1433442\n",
      "Epoch 2\n",
      "Training loss: 1.1300222, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1411917, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.026788, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.1411917, new_val_loss=1.1422396\n",
      "Epoch 3\n",
      "Training loss: 1.1288922, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1401577, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0274109, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.1401577, new_val_loss=1.1411917\n",
      "Epoch 4\n",
      "Training loss: 1.127775, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1391373, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0280478, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.1391373, new_val_loss=1.1401577\n",
      "Epoch 5\n",
      "Training loss: 1.1266696, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.138131, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0286955, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.138131, new_val_loss=1.1391373\n",
      "Epoch 6\n",
      "Training loss: 1.1255774, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1371381, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0293571, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.1371381, new_val_loss=1.138131\n",
      "Epoch 7\n",
      "Training loss: 1.1244974, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.136159, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0300326, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.136159, new_val_loss=1.1371381\n",
      "Epoch 8\n",
      "Training loss: 1.1234307, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1351932, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0307221, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.1351932, new_val_loss=1.136159\n",
      "Epoch 9\n",
      "Training loss: 1.1223758, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.134241, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0314256, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.134241, new_val_loss=1.1351932\n",
      "Epoch 10\n",
      "Training loss: 1.1213336, Training accuracy: 31.70731707317073 %\n",
      "Validation loss: 1.1333022, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.032143, Test accuracy: 53.333333333333336 %\n",
      "[->] Found new best model: old_val_loss=1.1333022, new_val_loss=1.134241\n",
      "Epoch 0\n",
      "Training loss: 1.1120493, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.060232, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.115082, Test accuracy: 40.0 %\n",
      "Epoch 1\n",
      "Training loss: 1.1113764, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.0618861, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.1152806, Test accuracy: 40.0 %\n",
      "Epoch 2\n",
      "Training loss: 1.1107166, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.0635546, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.1154933, Test accuracy: 40.0 %\n",
      "Epoch 3\n",
      "Training loss: 1.1100698, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.0652379, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.1157197, Test accuracy: 40.0 %\n",
      "Epoch 4\n",
      "Training loss: 1.1094356, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.0669357, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.1159599, Test accuracy: 40.0 %\n",
      "Epoch 5\n",
      "Training loss: 1.1088147, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.0686479, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.116214, Test accuracy: 40.0 %\n",
      "Epoch 0\n",
      "Training loss: 1.2450986, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0662764, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2316957, Test accuracy: 33.33333333333333 %\n",
      "Epoch 1\n",
      "Training loss: 1.2431786, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0655628, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2298636, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0655628, new_val_loss=1.0662764\n",
      "Epoch 2\n",
      "Training loss: 1.2412696, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0648595, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2280427, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0648595, new_val_loss=1.0655628\n",
      "Epoch 3\n",
      "Training loss: 1.2393715, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0641663, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2262328, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0641663, new_val_loss=1.0648595\n",
      "Epoch 4\n",
      "Training loss: 1.2374853, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0634832, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2244343, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0634832, new_val_loss=1.0641663\n",
      "Epoch 5\n",
      "Training loss: 1.2356102, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0628104, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2226471, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0628104, new_val_loss=1.0634832\n",
      "Epoch 6\n",
      "Training loss: 1.2337465, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0621482, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.220871, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0621482, new_val_loss=1.0628104\n",
      "Epoch 7\n",
      "Training loss: 1.2318943, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.061496, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2191063, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.061496, new_val_loss=1.0621482\n",
      "Epoch 8\n",
      "Training loss: 1.2300537, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0608544, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2173531, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0608544, new_val_loss=1.061496\n",
      "Epoch 9\n",
      "Training loss: 1.228224, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0602232, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2156111, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0602232, new_val_loss=1.0608544\n",
      "Epoch 10\n",
      "Training loss: 1.2264067, Training accuracy: 30.89430894308943 %\n",
      "Validation loss: 1.0596024, Validation accuracy: 61.53846153846154 %\n",
      "Test loss: 1.2138809, Test accuracy: 33.33333333333333 %\n",
      "[->] Found new best model: old_val_loss=1.0596024, new_val_loss=1.0602232\n",
      "Epoch 0\n",
      "Training loss: 1.1438376, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2471918, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1600811, Test accuracy: 26.666666666666668 %\n",
      "Epoch 1\n",
      "Training loss: 1.1427186, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2449012, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1589481, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2449012, new_val_loss=1.2471918\n",
      "Epoch 2\n",
      "Training loss: 1.1416119, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2426223, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1578273, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2426223, new_val_loss=1.2449012\n",
      "Epoch 3\n",
      "Training loss: 1.1405176, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2403553, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1567189, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2403553, new_val_loss=1.2426223\n",
      "Epoch 4\n",
      "Training loss: 1.1394354, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2381005, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1556227, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2381005, new_val_loss=1.2403553\n",
      "Epoch 5\n",
      "Training loss: 1.1383662, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2358577, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1545391, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2358577, new_val_loss=1.2381005\n",
      "Epoch 6\n",
      "Training loss: 1.137309, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2336273, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.153468, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2336273, new_val_loss=1.2358577\n",
      "Epoch 7\n",
      "Training loss: 1.1362648, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2314093, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1524097, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2314093, new_val_loss=1.2336273\n",
      "Epoch 8\n",
      "Training loss: 1.1352332, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2292036, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1513638, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2292036, new_val_loss=1.2314093\n",
      "Epoch 9\n",
      "Training loss: 1.1342143, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.22701, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1503309, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.22701, new_val_loss=1.2292036\n",
      "Epoch 10\n",
      "Training loss: 1.1332084, Training accuracy: 34.959349593495936 %\n",
      "Validation loss: 1.2248294, Validation accuracy: 23.076923076923077 %\n",
      "Test loss: 1.1493108, Test accuracy: 26.666666666666668 %\n",
      "[->] Found new best model: old_val_loss=1.2248294, new_val_loss=1.22701\n",
      "Epoch 0\n",
      "Training loss: 1.1139904, Training accuracy: 37.39837398373984 %\n",
      "Validation loss: 1.138641, Validation accuracy: 15.384615384615385 %\n",
      "Test loss: 1.1369042, Test accuracy: 20.0 %\n",
      "Epoch 1\n",
      "Training loss: 1.113592, Training accuracy: 37.39837398373984 %\n",
      "Validation loss: 1.138976, Validation accuracy: 15.384615384615385 %\n",
      "Test loss: 1.1368932, Test accuracy: 20.0 %\n",
      "Epoch 2\n",
      "Training loss: 1.1132035, Training accuracy: 37.39837398373984 %\n",
      "Validation loss: 1.1393218, Validation accuracy: 15.384615384615385 %\n",
      "Test loss: 1.1368922, Test accuracy: 20.0 %\n",
      "Epoch 3\n",
      "Training loss: 1.112823, Training accuracy: 37.39837398373984 %\n",
      "Validation loss: 1.1395441, Validation accuracy: 15.384615384615385 %\n",
      "Test loss: 1.1368186, Test accuracy: 20.0 %\n",
      "Epoch 4\n",
      "Training loss: 1.1124504, Training accuracy: 37.39837398373984 %\n",
      "Validation loss: 1.139777, Validation accuracy: 15.384615384615385 %\n",
      "Test loss: 1.1367542, Test accuracy: 20.0 %\n",
      "Epoch 5\n",
      "Training loss: 1.1120855, Training accuracy: 37.39837398373984 %\n",
      "Validation loss: 1.1400192, Validation accuracy: 15.384615384615385 %\n",
      "Test loss: 1.1366981, Test accuracy: 20.0 %\n",
      "Epoch 0\n",
      "Training loss: 1.1074635, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1079705, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0499427, Test accuracy: 60.0 %\n",
      "Epoch 1\n",
      "Training loss: 1.106742, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1071662, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0514878, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1071662, new_val_loss=1.1079705\n",
      "Epoch 2\n",
      "Training loss: 1.1060405, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1063831, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.053051, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1063831, new_val_loss=1.1071662\n",
      "Epoch 3\n",
      "Training loss: 1.1053588, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1056209, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0546318, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1056209, new_val_loss=1.1063831\n",
      "Epoch 4\n",
      "Training loss: 1.1046972, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1048796, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0562302, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1048796, new_val_loss=1.1056209\n",
      "Epoch 5\n",
      "Training loss: 1.1040548, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.104159, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0578461, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.104159, new_val_loss=1.1048796\n",
      "Epoch 6\n",
      "Training loss: 1.1034323, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1034589, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0594792, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1034589, new_val_loss=1.104159\n",
      "Epoch 7\n",
      "Training loss: 1.1028281, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1027839, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0611221, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1027839, new_val_loss=1.1034589\n",
      "Epoch 8\n",
      "Training loss: 1.1022428, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1021289, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0627807, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1021289, new_val_loss=1.1027839\n",
      "Epoch 9\n",
      "Training loss: 1.1016759, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1014932, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0644563, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1014932, new_val_loss=1.1021289\n",
      "Epoch 10\n",
      "Training loss: 1.101127, Training accuracy: 30.081300813008134 %\n",
      "Validation loss: 1.1008766, Validation accuracy: 30.76923076923077 %\n",
      "Test loss: 1.0661483, Test accuracy: 60.0 %\n",
      "[->] Found new best model: old_val_loss=1.1008766, new_val_loss=1.1014932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[1.1058107614517212, 1.1048879623413086, 1.103974461555481, 1.103070616722107, 1.1021769046783447, 1.1012932062149048, 1.1004186868667603, 1.0995550155639648, 1.0987004041671753, 1.0978556871414185  …  1.106742024421692, 1.1060404777526855, 1.1053588390350342, 1.1046972274780273, 1.1040548086166382, 1.103432297706604, 1.1028281450271606, 1.1022428274154663, 1.1016758680343628, 1.101127028465271], Any[0.34146341463414637, 0.3333333333333333, 0.3252032520325203, 0.34146341463414637, 0.3170731707317073, 0.3008130081300813, 0.3089430894308943, 0.34959349593495936, 0.37398373983739835, 0.3008130081300813], Any[1.0411938428878784, 1.0421404838562012, 1.0430951118469238, 1.0440577268600464, 1.0450286865234375, 1.046007752418518, 1.0469954013824463, 1.047991156578064, 1.0487675666809082, 1.0495514869689941  …  1.0514878034591675, 1.0530509948730469, 1.0546318292617798, 1.0562301874160767, 1.0578460693359375, 1.0594792366027832, 1.0611220598220825, 1.062780737876892, 1.0644563436508179, 1.066148281097412], Any[0.4, 0.3333333333333333, 0.4, 0.3333333333333333, 0.5333333333333333, 0.4, 0.3333333333333333, 0.26666666666666666, 0.2, 0.6], Any[1.1474885940551758, 1.1462607383728027, 1.1450423002243042, 1.143833041191101, 1.1426331996917725, 1.1414427757263184, 1.1402617692947388, 1.1390904188156128, 1.1378482580184937, 1.136615514755249  …  1.1071661710739136, 1.1063830852508545, 1.1056208610534668, 1.10487961769104, 1.1041589975357056, 1.1034588813781738, 1.1027839183807373, 1.1021288633346558, 1.101493239402771, 1.1008765697479248], Any[0.23076923076923078, 0.38461538461538464, 0.3076923076923077, 0.23076923076923078, 0.3076923076923077, 0.6153846153846154, 0.6153846153846154, 0.23076923076923078, 0.15384615384615385, 0.3076923076923077])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First train: ANNs\n",
    "\n",
    "symbol = :ANN\n",
    "\n",
    "hyperparameters = Dict()\n",
    "hyperparameters[\"architecture\"]=[2, 5]\n",
    "hyperparameters[\"lr\"]=0.001\n",
    "hyperparameters[\"val_ratio\"]=0.1\n",
    "hyperparameters[\"epochs\"]=10                \n",
    "hyperparameters[\"early_stop_epochs\"]=5\n",
    "hyperparameters[\"n_train\"]=1\n",
    "\n",
    "modelCrossValidation(symbol, hyperparameters, inputs, targets, cross_val_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7ded7",
   "metadata": {},
   "source": [
    "### Learn Julia\n",
    "\n",
    "In this assignment, it is necessary to pass parameters which are dependent on the model. To do this, the simplest way is to create a variable of type Dictionary (actually the type is `Dict`) which works in a similar way to Python. For example, to specify the parameters of an SVM, you could create a variable as follows:\n",
    "\n",
    "```Julia\n",
    "parameters = Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"gamma\" => 2, \"C\" => 1);\n",
    "```\n",
    "\n",
    "Another way of defining such a variable could be the following:\n",
    "\n",
    "```Julia\n",
    "parameters = Dict();\n",
    "\n",
    "parameters[\"kernel\"] = \"rbf\";\n",
    "parameters[\"kernelDegree\"] = 3;\n",
    "parameters[\"kernelGamma\"] = 2;\n",
    "parameters[\"C\"] = 1;\n",
    "```\n",
    "\n",
    "Once inside the function to be developed, the model parameters can be used to create the model objet as follows:\n",
    "\n",
    "```Julia\n",
    "model = SVC(kernel=parameters[\"kernel\"], \n",
    "    degree=parameters[\"kernelDegree\"], \n",
    "    gamma=parameters[\"kernelGamma\"], \n",
    "    C=parameters[\"C\"]);\n",
    "```\n",
    "\n",
    "In the same way, something similar could be done for decision trees and kNN.\n",
    "\n",
    "Another type of Julia that may be interesting for this assignment is the `Symbol` type. An object of this type can be any symbol you want, simply by typing its name after a colon (\":\"). In this practice, you can use it to indicate which model you want to train, for example `:ANN`, `:SVM`, `:DecisionTree` or `:kNN`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
